{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a7e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dill\n",
    "\n",
    "def print_key_tree(obj, prefix=\"\", max_depth=3, current_depth=0):\n",
    "    \"\"\"\n",
    "    递归打印嵌套字典/对象的键结构树\n",
    "    \"\"\"\n",
    "    if current_depth > max_depth:\n",
    "        print(f\"{prefix}...\")\n",
    "        return\n",
    "        \n",
    "    if isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            type_name = type(value).__name__\n",
    "            if hasattr(value, '__len__') and not isinstance(value, (str, bytes)):\n",
    "                if hasattr(value, 'shape'):  # Tensor\n",
    "                    print(f\"{prefix}{key}: {type_name} (shape: {value.shape})\")\n",
    "                else:  # 其他容器类型\n",
    "                    print(f\"{prefix}{key}: {type_name} (len: {len(value)})\")\n",
    "                    if current_depth < max_depth and isinstance(value, dict):\n",
    "                        print_key_tree(value, prefix + \"  \", max_depth, current_depth + 1)\n",
    "                    elif current_depth < max_depth and hasattr(value, '__getitem__'):\n",
    "                        # 尝试查看前几个键\n",
    "                        try:\n",
    "                            if len(value) > 0:\n",
    "                                print(f\"{prefix}  Sample keys: {list(value.keys())[:5]}\")\n",
    "                        except:\n",
    "                            pass\n",
    "            else:\n",
    "                print(f\"{prefix}{key}: {type_name}\")\n",
    "                # 如果是复合对象，递归查看\n",
    "                if current_depth < max_depth and hasattr(value, '__dict__'):\n",
    "                    print_key_tree(value.__dict__, prefix + \"  \", max_depth, current_depth + 1)\n",
    "    \n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        print_key_tree(obj.__dict__, prefix, max_depth, current_depth)\n",
    "\n",
    "# 加载checkpoint\n",
    "path = \"/data/yixiang/workspace/SRCB-DexGraspVLA-Project/checkpoint/mixed_4_data/epoch=120.ckpt\"\n",
    "payload = torch.load(path, pickle_module=dill, weights_only=False)\n",
    "\n",
    "print(\"=== Checkpoint Structure ===\")\n",
    "print_key_tree(payload)\n",
    "\n",
    "# 详细查看state_dict\n",
    "if 'state_dicts' in payload:\n",
    "    print(\"\\n=== State Dict Structure ===\")\n",
    "    print(f\"Total parameters: {len(payload['state_dicts']['model'])}\")\n",
    "    \n",
    "    # 按模块分组显示\n",
    "    modules = {}\n",
    "    for key in payload['state_dicts']['model'].keys():\n",
    "        module_name = key.split('.')[0] if '.' in key else key\n",
    "        if module_name not in modules:\n",
    "            modules[module_name] = []\n",
    "        modules[module_name].append(key)\n",
    "    \n",
    "    for module_name, keys in modules.items():\n",
    "        print(f\"{module_name}: {len(keys)} parameters\")\n",
    "        # 显示前3个参数作为示例\n",
    "        for key in keys[:3]:\n",
    "            tensor = payload['state_dicts']['model'][key]\n",
    "            if hasattr(tensor, 'shape'):\n",
    "                print(f\"  {key}: {tensor.shape}\")\n",
    "        if len(keys) > 3:\n",
    "            print(f\"  ... and {len(keys)-3} more\")\n",
    "\n",
    "# 查看其他元信息\n",
    "print(\"\\n=== Other Metadata ===\")\n",
    "for key in payload.keys():\n",
    "    if key != 'state_dicts':\n",
    "        value = payload[key]\n",
    "        if isinstance(value, (str, int, float, bool)) or value is None:\n",
    "            print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(f\"{key}: {type(value).__name__} ({str(value)[:100]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ec40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, parent_dir)\n",
    "from typing import Dict, Callable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import zarr\n",
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import dill\n",
    "import hydra\n",
    "from dexgraspvla.controller.policy.dexgraspvla_controller import DexGraspVLAController\n",
    "from scripts.utils.profile_utils import profile_class\n",
    "\n",
    "def create_dummy_payload(model, action_shape):\n",
    "    \"\"\"\n",
    "    创建包含完整 model 参数（包括 normalizer）的 payload\n",
    "    \"\"\"\n",
    "    # 先获取原始 model 结构，含 normalizer keys\n",
    "    model_state_dict = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        model_state_dict[name] = torch.randn_like(param)\n",
    "\n",
    "    # 把 normalizer 的模拟参数插入进去（如果它们本来不在 named_parameters 里）\n",
    "    # 这取决于你 LinearNormalizer 是否继承了 nn.Module 并注册了参数\n",
    "    from torch import nn\n",
    "\n",
    "    if hasattr(action_shape, '__getitem__'):\n",
    "        action_dim = action_shape[-1]\n",
    "    else:\n",
    "        action_dim = int(action_shape)\n",
    "\n",
    "    fake_min = torch.full((action_dim,), -1.0)\n",
    "    fake_max = torch.full((action_dim,), 1.0)\n",
    "    fake_mean = torch.zeros((action_dim,))\n",
    "    fake_std = torch.ones((action_dim,))\n",
    "    scale = torch.full((action_dim,), 2.0)\n",
    "    offset = torch.full((action_dim,), -1.0)\n",
    "\n",
    "    # 注意这里的 keys 必须与你在 model.state_dict() 看到的一模一样！\n",
    "    normalizer_dummy_params = {\n",
    "        'normalizer.params_dict.action.offset': offset,\n",
    "        'normalizer.params_dict.action.scale': scale,\n",
    "        'normalizer.params_dict.action.input_stats.max': fake_max,\n",
    "        'normalizer.params_dict.action.input_stats.mean': fake_mean,\n",
    "        'normalizer.params_dict.action.input_stats.min': fake_min,\n",
    "        'normalizer.params_dict.action.input_stats.std': fake_std,\n",
    "\n",
    "        'normalizer.params_dict.right_state.offset': offset,\n",
    "        'normalizer.params_dict.right_state.scale': scale,\n",
    "        'normalizer.params_dict.right_state.input_stats.max': fake_max,\n",
    "        'normalizer.params_dict.right_state.input_stats.mean': fake_mean,\n",
    "        'normalizer.params_dict.right_state.input_stats.min': fake_min,\n",
    "        'normalizer.params_dict.right_state.input_stats.std': fake_std,\n",
    "    }\n",
    "\n",
    "    # 合并进 model_state_dict\n",
    "    model_state_dict.update(normalizer_dummy_params)\n",
    "\n",
    "    payload = {\n",
    "        \"state_dicts\": {\n",
    "            \"model\": model_state_dict\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return payload\n",
    "\n",
    "def load_config(main_config_path, task_config_path):\n",
    "    \"\"\"\n",
    "    Load main configuration file and its referenced configuration files\n",
    "\n",
    "    Args:\n",
    "        config_path: Configuration file root directory\n",
    "        config_name: Main configuration file name (without .yaml)\n",
    "    \"\"\"\n",
    "    def now_resolver(pattern: str):\n",
    "        \"\"\"Handle ${now:} time formatting\"\"\"\n",
    "        return datetime.now().strftime(pattern)\n",
    "\n",
    "    OmegaConf.register_new_resolver(\"now\", now_resolver, replace=True)\n",
    "    OmegaConf.register_new_resolver(\"eval\", eval, replace=True)\n",
    "\n",
    "    # Create default configuration\n",
    "    default_cfg = OmegaConf.create({\n",
    "        \"hydra\": {\n",
    "            \"job\": {\n",
    "                \"num\": 0,  # Provide default value\n",
    "                \"override_dirname\": \"${name}\"\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "\n",
    "    # Load main configuration file\n",
    "    cfg = OmegaConf.load(main_config_path)\n",
    "\n",
    "    # Merge default configuration\n",
    "    cfg = OmegaConf.merge(default_cfg, cfg)\n",
    "    task_cfg = OmegaConf.load(task_config_path)\n",
    "    cfg[\"task\"] = task_cfg\n",
    "\n",
    "    # Parse all variable references\n",
    "    OmegaConf.resolve(cfg)\n",
    "\n",
    "    return cfg\n",
    "def load_zarr_data(zarr_path):\n",
    "    \"\"\"加载Zarr数据集\"\"\"\n",
    "    try:\n",
    "        f = zarr.open(zarr_path)\n",
    "        print(list(f['data'].keys()))\n",
    "        rgbm_data = f['data/rgbm'][:]\n",
    "        action = f['data/action'][:]\n",
    "        right_cam_img = f['data/right_cam_img'][:]\n",
    "        right_state = f['data/right_state'][:]\n",
    "        episode_ends = np.insert(f['meta/episode_ends'][:], 0, 0)\n",
    "        return rgbm_data, action, right_cam_img, right_state, episode_ends\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading Zarr data: {str(e)}\")\n",
    "\n",
    "def update_array(existing_array, new_array):\n",
    "    # Create new array to store updated data\n",
    "    updated_array = np.empty_like(existing_array)\n",
    "\n",
    "    # Move the previous array's last item to the second position\n",
    "    for i in range(0, existing_array.shape[0]):\n",
    "        if i < existing_array.shape[0]-1:\n",
    "            updated_array[i, ...] = existing_array[i+1, ...]\n",
    "        else:\n",
    "            # Add new array to the last position of the first dimension\n",
    "            updated_array[i, ...] = new_array\n",
    "\n",
    "    return updated_array\n",
    "\n",
    "def dict_apply(\n",
    "        x: Dict[str, torch.Tensor], \n",
    "        func: Callable[[torch.Tensor], torch.Tensor]\n",
    "        ) -> Dict[str, torch.Tensor]:\n",
    "    result = dict()\n",
    "    for key, value in x.items():\n",
    "        if isinstance(value, dict):\n",
    "            result[key] = dict_apply(value, func)\n",
    "        else:\n",
    "            result[key] = func(value)\n",
    "    return result\n",
    "\n",
    "def load_inference_config(config_path):\n",
    "    \"\"\"Load system configuration from YAML file\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "class VLAController:\n",
    "    def __init__(self, config, vla_cfg, model:DexGraspVLAController, payload = {}) -> None:\n",
    "        self.config = config\n",
    "        self.cfg = vla_cfg\n",
    "        resolution = self.config['cameras']['right_hand_cameras']['resolution']\n",
    "        self.right_first_color_image_buffer = np.zeros((self.cfg.n_obs_steps, resolution[1], resolution[0], 3))\n",
    "        self.third_color_image_buffer = np.zeros((self.cfg.n_obs_steps, resolution[1], resolution[0], 4))\n",
    "        self.state_buffer = np.zeros((self.cfg.n_obs_steps, 13))\n",
    "        self.time_step = 0\n",
    "        self.device = 'cuda:0'\n",
    "        self.model = model\n",
    "        \n",
    "        if payload:\n",
    "            model_state = payload.get('state_dicts', {}).get('model')\n",
    "            if model_state:\n",
    "                self.model.load_state_dict(model_state)\n",
    "            else:\n",
    "                print(\"Model state dict not found in payload.\")\n",
    "        else:\n",
    "            print(\"Payload is None or empty.\")\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def predict_action(self, state, right_first_color_image, third_color_image_with_mask):\n",
    "        obs = self.get_obs(state, right_first_color_image, third_color_image_with_mask)\n",
    "        attn_map_output_path = None\n",
    "        self.time_step += 1\n",
    "        obs_dict_np = self.process_obs(env_obs=obs, shape_meta=self.cfg.task.shape_meta)\n",
    "        obs_dict = dict_apply(obs_dict_np, \n",
    "                lambda x: torch.from_numpy(x).unsqueeze(0).to(self.device))\n",
    "        # print(\"test==============:2\")\n",
    "        with torch.no_grad():\n",
    "            action_pred = self.model.predict_action(obs_dict, attn_map_output_path)\n",
    "            # print(\"test==============:3\")\n",
    "            action = action_pred[0].detach().to('cpu').numpy()\n",
    "        # print(\"test==============:\", action)\n",
    "        return action\n",
    "    \n",
    "    def get_obs(self, state, right_first_color_image, third_color_image_with_mask):\n",
    "        # self.show_and_save_image_with_mask(self.third_color_image, mask, \"/data/dingzher/DexGrasp_Demo/SRCB-DexVLA/temp_1\")\n",
    "        self.right_first_color_image_buffer = update_array(\n",
    "            self.right_first_color_image_buffer, \n",
    "            right_first_color_image\n",
    "        )\n",
    "        # self.show_and_save_image_with_mask(self.right_first_color_image, mask, \"/data/dingzher/DexGrasp_Demo/SRCB-DexVLA/temp_1\")\n",
    "        self.third_color_image_buffer = update_array(\n",
    "            self.third_color_image_buffer, \n",
    "            third_color_image_with_mask\n",
    "        )\n",
    "        self.state_buffer = update_array(self.state_buffer, state)\n",
    "        print(f\"state_value: {state}\")\n",
    "        # input(\"check the input\")\n",
    "        obs = {\"right_cam_img\": self.right_first_color_image_buffer, \"rgbm\": self.third_color_image_buffer, \"right_state\": self.state_buffer}\n",
    "        return obs\n",
    "    \n",
    "    def process_obs(self, env_obs, shape_meta):\n",
    "        \"\"\"Get observation dictionary, using torch for image processing\"\"\"\n",
    "        obs_dict_np = {}\n",
    "        obs_shape_meta = shape_meta['obs']\n",
    "        \n",
    "        for key, attr in obs_shape_meta.items():\n",
    "            type = attr.get('type', 'low_dim')\n",
    "            shape = attr.get('shape')\n",
    "\n",
    "            if type == 'rgb':\n",
    "                imgs_in = env_obs[key]\n",
    "                rgb = torch.from_numpy(imgs_in[..., :3]).float()  # [T, H, W, 3]\n",
    "                rgb = rgb.permute(0, 3, 1, 2)  # [T, 3, H, W]\n",
    "                # Scale image\n",
    "                rgb = F.interpolate(\n",
    "                    rgb / 255.0,\n",
    "                    size=(shape[1], shape[2]),\n",
    "                    mode='bilinear',\n",
    "                    align_corners=False\n",
    "                )\n",
    "                obs_dict_np[key] = rgb.numpy()\n",
    "\n",
    "            elif type == 'rgbm':  # Process mask image\n",
    "                imgs_in = env_obs[key]\n",
    "                # Convert to torch tensor and adjust dimensions\n",
    "                rgb = torch.from_numpy(imgs_in[..., :3]).float()  # [T, H, W, 3]\n",
    "                mask = torch.from_numpy(imgs_in[..., 3:]).float()\n",
    "                # Adjust channel order\n",
    "                rgb = rgb.permute(0, 3, 1, 2)  # [T, 3, H, W]\n",
    "                # Scale RGB\n",
    "                rgb = F.interpolate(\n",
    "                    rgb / 255.0,\n",
    "                    size=(shape[1], shape[2]),  # Use the size specified in shape_meta\n",
    "                    mode='bilinear',\n",
    "                    align_corners=False\n",
    "                )\n",
    "                # Process mask\n",
    "                mask = mask.permute(0, 3, 1, 2)  # [T, 1, H, W]\n",
    "                mask = F.interpolate(\n",
    "                    mask,\n",
    "                    size=(shape[1], shape[2]),\n",
    "                    mode='nearest'\n",
    "                )\n",
    "                mask = (mask > 0.5).float()\n",
    "                # Combine RGB and mask\n",
    "                out_imgs = torch.cat([rgb, mask], dim=1)  # [T, 4, H, W]\n",
    "                obs_dict_np[key] = out_imgs.numpy()\n",
    "\n",
    "            elif type == 'low_dim':\n",
    "                obs_dict_np[key] = env_obs[key].astype(np.float32)\n",
    "        \n",
    "        return obs_dict_np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76fb2e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/data/yixiang/workspace/SRCB-DexGraspVLA-Project/checkpoint/mixed_4_data/epoch=120.ckpt\"\n",
    "payload = torch.load(path, pickle_module=dill, weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf111bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['action', 'rgbm', 'right_cam_img', 'right_state']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path = '/data/dingzher/DexGrasp_Demo/SRCB-DexVLA/zarr_data_transfer/output_data_20250610_single_bowl.zarr/'\n",
    "rgbm_data, action, right_cam_img, right_state, episode_ends = load_zarr_data(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db7d3c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8908, 480, 640, 4)\n",
      "(64, 13)\n",
      "(8908, 480, 640, 3)\n",
      "(8908, 13)\n"
     ]
    }
   ],
   "source": [
    "print(rgbm_data.shape)\n",
    "print(action.shape)\n",
    "print(right_cam_img.shape)\n",
    "print(right_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baab39cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hydra': {'job': {'num': 0, 'override_dirname': 'train_dexgraspvla_controller'}, 'run': {'dir': 'data/outputs/2025.08.05/15.35_train_dexgraspvla_controller_grasp'}, 'sweep': {'dir': 'data/outputs/2025.08.05/15.35_train_dexgraspvla_controller_grasp', 'subdir': 0}}, 'defaults': ['_self_', {'task': 'grasp'}], 'name': 'train_dexgraspvla_controller', '_target_': 'dexgraspvla.controller.workspace.train_dexgraspvla_controller_workspace.TrainDexGraspVLAControllerWorkspace', 'task_name': 'grasp', 'shape_meta': {'obs': {'right_cam_img': {'shape': [3, 518, 518], 'type': 'rgb', 'horizon': 1}, 'rgbm': {'shape': [4, 518, 518], 'type': 'rgbm', 'horizon': 1}, 'right_state': {'shape': [13], 'type': 'low_dim', 'horizon': 1}}, 'action': {'shape': [13], 'horizon': 64}}, 'exp_name': 'default', 'n_action_steps': 64, 'n_obs_steps': 1, 'n_latency_steps': 0, 'dataset_obs_steps': 1, 'past_action_visible': False, 'keypoint_visible_rate': 1.0, 'obs_as_cond': True, 'policy': {'_target_': 'dexgraspvla.controller.policy.dexgraspvla_controller.DexGraspVLAController', 'shape_meta': {'obs': {'right_cam_img': {'shape': [3, 518, 518], 'type': 'rgb', 'horizon': 1}, 'rgbm': {'shape': [4, 518, 518], 'type': 'rgbm', 'horizon': 1}, 'right_state': {'shape': [13], 'type': 'low_dim', 'horizon': 1}}, 'action': {'shape': [13], 'horizon': 64}}, 'noise_scheduler': {'_target_': 'diffusers.DDIMScheduler', 'num_train_timesteps': 50, 'beta_start': 0.0001, 'beta_end': 0.02, 'beta_schedule': 'squaredcos_cap_v2', 'clip_sample': True, 'set_alpha_to_one': True, 'steps_offset': 0, 'prediction_type': 'epsilon'}, 'obs_encoder': {'_target_': 'dexgraspvla.controller.model.vision.obs_encoder.ObsEncoder', 'shape_meta': {'obs': {'right_cam_img': {'shape': [3, 518, 518], 'type': 'rgb', 'horizon': 1}, 'rgbm': {'shape': [4, 518, 518], 'type': 'rgbm', 'horizon': 1}, 'right_state': {'shape': [13], 'type': 'low_dim', 'horizon': 1}}, 'action': {'shape': [13], 'horizon': 64}}, 'model_config': {'head': {'model_type': 'dinov2_vitb14', 'local_weights_path': None}, 'wrist': {'model_type': 'dinov2_vitl14', 'local_weights_path': None}}}, 'num_inference_steps': 16, 'n_layer': 12, 'n_head': 8, 'p_drop_attn': 0.1, 'use_attn_mask': False, 'start_ckpt_path': None}, 'ema': {'_target_': 'dexgraspvla.controller.model.diffusion.ema_model.EMAModel', 'update_after_step': 0, 'inv_gamma': 1.0, 'power': 0.75, 'min_value': 0.0, 'max_value': 0.9999}, 'dataloader': {'batch_size': 48, 'num_workers': 8, 'shuffle': True, 'pin_memory': True, 'persistent_workers': True}, 'val_dataloader': {'batch_size': 48, 'num_workers': 8, 'shuffle': False, 'pin_memory': True, 'persistent_workers': True}, 'optimizer': {'lr': 0.0001, 'weight_decay': 0.0001, 'betas': [0.95, 0.999]}, 'training': {'device': 'cuda:0', 'seed': 42, 'debug': False, 'resume': False, 'lr_scheduler': 'cosine', 'lr_warmup_steps': 2000, 'num_epochs': 125, 'gradient_accumulate_every': 1, 'use_ema': False, 'rollout_every': 10, 'checkpoint_every': 1, 'val_every': 10000, 'sample_every': 10, 'gen_attn_map': True, 'max_train_steps': None, 'max_val_steps': None, 'tqdm_interval_sec': 1.0}, 'logging': {'project': 'train_dexgraspvla_controller', 'resume': False, 'mode': 'online', 'name': '2025.08.05-15.35_train_dexgraspvla_controller_grasp', 'tags': ['train_dexgraspvla_controller', 'grasp', 'default'], 'id': None, 'group': None}, 'checkpoint': {'topk': {'monitor_key': 'train_loss', 'mode': 'min', 'k': 1, 'format_str': 'epoch={epoch:04d}-train_loss={train_loss:.3f}.ckpt'}, 'save_last_ckpt': False, 'save_last_snapshot': False}, 'multi_run': {'run_dir': 'data/outputs/2025.08.05/15.35_train_dexgraspvla_controller_grasp', 'wandb_name_base': '2025.08.05-15.35_train_dexgraspvla_controller_grasp'}, 'task': {'name': 'grasp', 'image_shape': [3, 518, 518], 'mask_image_shape': [4, 518, 518], 'dataset_paths': ['data/grasp_demo_example'], 'shape_meta': {'obs': {'right_cam_img': {'shape': [3, 518, 518], 'type': 'rgb', 'horizon': 1}, 'rgbm': {'shape': [4, 518, 518], 'type': 'rgbm', 'horizon': 1}, 'right_state': {'shape': [13], 'type': 'low_dim', 'horizon': 1}}, 'action': {'shape': [13], 'horizon': 64}}, 'env_runner': {'_target_': 'controller.env_runner.real_grasp_image_runner.RealGraspImageRunner'}, 'dataset': {'_target_': 'controller.dataset.mask_image_dataset.MaskImageDataset', 'zarr_paths': ['data/grasp_demo_example'], 'horizon': 64, 'pad_before': 0, 'pad_after': 63, 'seed': 42, 'val_ratio': 0}}}\n",
      "{'robot': {'base_frame': 'base_link_frd', 'operation_frame': 'torso_link4', 'dof_limits': {'lower': [-3.1, -2.268, -3.1, -2.355, -3.1, -2.233, -6.28], 'upper': [3.1, 2.268, 3.1, 2.355, 3.1, 2.233, 6.28]}, 'hands': {'left': {'ip': '192.168.3.210', 'port': 6000, 'read_service': '/inspire_hand_modbus/get_angle_act', 'move_service': '/inspire_hand_modbus/set_angle', 'default_open': [1000, 1000, 1000, 1000, 1000, 1000], 'default_standby': [755, 805, 820, 860, 1000, 410], 'default_grasp': [525, 510, 495, 450, 500, 0], 'default_speed': [200, 200, 200, 200, 200, 200]}, 'right': {'ip': '192.168.3.44', 'port': 6000, 'read_topic': '/inspire_hand_modbus/get_angle_act', 'move_topic': '/inspire_hand_modbus/set_angle', 'default_open': [1000, 1000, 1000, 1000, 1000, 1000], 'default_standby': [755, 805, 820, 860, 1000, 250], 'default_grasp': [10, 150, 250, 450, 500, 0], 'default_speed': [200, 200, 200, 200, 200, 200]}}, 'arms': {'left': {'ip': None, 'read_topic': '/hdas/feedback_arm_left', 'read_ee_topic': '/motion_control/pose_ee_arm_left', 'move_topic': '/motion_target/target_joint_state_arm_left', 'move_ee_topic': '/motion_target/target_pose_arm_left', 'init_qpos': [-0.6, 0.6, -0.5, -1.3, 0, 0, 0], 'init_ee': [0.526, 0.336, 0.1214, 0.1876, -0.7094, -0.32, 0.5989], 'default_qvel': [1, 1, 1, 1, 2, 2, 2]}, 'right': {'ip': None, 'read_topic': '/hdas/feedback_arm_right', 'read_ee_topic': '/motion_control/pose_ee_arm_right', 'move_topic': '/motion_target/target_joint_state_arm_right', 'move_ee_topic': '/motion_target/target_pose_arm_right', 'init_qpos': [-0.6, -0.6, 0.5, -1.3, 1.3, 0, 0], 'init_ee': [0.526, -0.336, 0.1214, -0.578, -0.451, 0.6181, 0.2833], 'default_qvel': [1, 1, 1, 1, 2, 2, 2], 'placement_joint': [-1.1612766, -0.66617024, 1.4272342, -1.4314893, 1.4136167, -0.13574481, -0.53234053], 'return_medium_joint': [0.58597687, -0.72916365, 0.20774654, -1.24625233, -2.82404745, 1.29088039, 2.68651541]}}, 'torso': {'move_topic': '/motion_target/target_joint_state_torso', 'init_qpos': [0.3, -0.4, -0.5, 0], 'default_qvel': [0.25, 0.25, 0.25, 0.25]}}, 'cameras': {'head_cameras': {'frame': 'zed_link', 'topic': '/hdas/camera_head/rgb/image_rect_color', 'depth_topic': '/hdas/camera_head/depth/depth_registered', 'depth_info_topic': '/hdas/camera_head/depth/camera_info', 'pointcloud_topic': '/hdas/camera_head/point_cloud/cloud_registered', 'resolution': [640, 480]}, 'left_hand_cameras': {'topic': '/hdas/camera_wrist_left/color/image_raw', 'resolution': [640, 480]}, 'right_hand_cameras': {'topic': '/hdas/camera_wrist_right/color/image_raw', 'resolution': [640, 480]}}, 'sam': {'checkpoint': 'checkpoint/sam/sam_vit_h_4b8939.pth', 'model_type': 'vit_h'}, 'planner': {'model_path_VL': '/data/yixiang/SRCB-DexVLA/checkpoint/Qwen2.5-VL-7B-Instruct', 'model_path_Omni': '/data/model/Qwen/Qwen2.5-Omni-7B'}, 'logging': {'path': 'log/', 'exp_name': 'demo'}, 'control': {'arm_trajectory': {'interpolation_num': 20, 'position_error_threshold': 0.03}, 'monitor': {'max_episode_duration': 100}}, 'visualization': {'bbox': {'color': [0.8, 0.2, 0.2], 'linewidth': 2}, 'mask': {'color': [[0.1, 0.5, 1, 0.6], [0.95, 0.4, 0.5, 0.6], [0.3, 0.9, 0.7, 0.55], [0.9, 0.7, 0.1, 0.65], [0.7, 0.5, 0.9, 0.6], [0.9, 0.45, 0.1, 0.6], [0.2, 0.8, 0.8, 0.55], [0.8, 0.3, 0.6, 0.65], [0.4, 0.4, 0.4, 0.5], [0.5, 0.6, 0.2, 0.6], [0.8, 0.2, 0.9, 0.55]]}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "current_dir = os.getcwd()  # 当前工作目录\n",
    "main_config_path = os.path.join(current_dir, '../src/dexgraspvla/controller', 'config', 'train_dexgraspvla_controller_workspace.yaml')\n",
    "task_config_path = os.path.join(current_dir, '../src/dexgraspvla/controller', 'config', 'task', 'grasp.yaml')\n",
    "\n",
    "vla_cfg = load_config(\n",
    "    main_config_path=main_config_path,\n",
    "    task_config_path=task_config_path\n",
    ")\n",
    "print(vla_cfg)\n",
    "\n",
    "inf_cfg = load_inference_config('/data/shiqi/SRCB-DexGraspVLA-Project/config.yaml')\n",
    "print(inf_cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8408115b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n",
      "Using cache found in /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = hydra.utils.instantiate(vla_cfg.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a96f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_payload = create_dummy_payload(model, vla_cfg['shape_meta'][\"action\"][\"shape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c688008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vla_controller=  VLAController(inf_cfg,vla_cfg, model, dummy_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dff4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbm_data[...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a72f08c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_value: [[-0.41853124 -0.33612895  0.56479067 -0.5503908   0.34763214  0.19313775\n",
      "  -0.08707141  0.512       0.61        0.638       0.718       0.994\n",
      "  -0.526     ]]\n",
      "\n",
      "=== Profile results for predict_action ===\n",
      "         56467 function calls (46336 primitive calls) in 0.343 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.343    0.343 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/policy/dexgraspvla_controller.py:162(predict_action)\n",
      "  5037/16    0.003    0.000    0.336    0.021 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1732(_wrapped_call_impl)\n",
      "  5037/16    0.006    0.000    0.336    0.021 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1740(_call_impl)\n",
      "        1    0.007    0.007    0.303    0.303 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/policy/dexgraspvla_controller.py:112(solve_ode)\n",
      "       15    0.001    0.000    0.296    0.020 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/model/diffusion/transformer_for_action_diffusion.py:391(forward)\n",
      "      180    0.006    0.000    0.290    0.002 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/model/diffusion/transformer_for_action_diffusion.py:228(forward)\n",
      "     1275    0.001    0.000    0.180    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/timm/layers/norm.py:150(forward)\n",
      "     1275    0.001    0.000    0.179    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/timm/layers/fast_norm.py:133(fast_rms_norm)\n",
      "     1275    0.117    0.000    0.178    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/timm/layers/fast_norm.py:111(rms_norm)\n",
      "      180    0.002    0.000    0.084    0.000 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/model/diffusion/transformer_for_action_diffusion.py:121(forward)\n",
      "      180    0.002    0.000    0.075    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/timm/models/vision_transformer.py:86(forward)\n",
      "      180    0.001    0.000    0.055    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/timm/layers/mlp.py:41(forward)\n",
      "     1275    0.047    0.000    0.047    0.000 {built-in method torch.var}\n",
      "      219    0.000    0.000    0.043    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/activation.py:733(forward)\n",
      "      219    0.043    0.000    0.043    0.000 {built-in method torch._C._nn.gelu}\n",
      "        1    0.000    0.000    0.040    0.040 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/model/vision/obs_encoder.py:211(forward)\n",
      "     1470    0.002    0.000    0.038    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:124(forward)\n",
      "     1470    0.035    0.000    0.035    0.000 {built-in method torch._C._nn.linear}\n",
      "        1    0.023    0.023    0.031    0.031 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/model/vision/obs_encoder.py:184(forward_wrist)\n",
      "        2    0.000    0.000    0.015    0.007 /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:298(get_intermediate_layers)\n",
      "        2    0.000    0.000    0.015    0.007 /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:272(_get_intermediate_layers_not_chunked)\n",
      "       36    0.000    0.000    0.014    0.000 /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:252(forward)\n",
      "       36    0.000    0.000    0.014    0.000 /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:89(forward)\n",
      "     1275    0.013    0.000    0.013    0.000 {built-in method torch.rsqrt}\n",
      "       36    0.000    0.000    0.010    0.000 /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:90(attn_residual_func)\n",
      "       36    0.000    0.000    0.009    0.000 /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:73(forward)\n",
      "        1    0.000    0.000    0.009    0.009 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/model/vision/obs_encoder.py:148(forward_head)\n",
      "      360    0.006    0.000    0.006    0.000 {built-in method torch._C._nn.scaled_dot_product_attention}\n",
      "       36    0.000    0.000    0.005    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py:194(memory_efficient_attention)\n",
      "       36    0.000    0.000    0.005    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py:462(_memory_efficient_attention)\n",
      "       36    0.000    0.000    0.005    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py:480(_memory_efficient_attention_forward)\n",
      "     9689    0.004    0.000    0.004    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1918(__getattr__)\n",
      "       36    0.000    0.000    0.004    0.000 /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:93(ffn_residual_func)\n",
      "       36    0.000    0.000    0.003    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py:126(_dispatch_fw)\n",
      "      831    0.000    0.000    0.003    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/dropout.py:69(forward)\n",
      "       36    0.000    0.000    0.003    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py:55(_run_priority_list)\n",
      "       15    0.000    0.000    0.003    0.000 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/model/diffusion/transformer_for_action_diffusion.py:55(forward)\n",
      "      831    0.001    0.000    0.003    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/functional.py:1401(dropout)\n",
      "       36    0.000    0.000    0.002    0.000 /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/mlp.py:34(forward)\n",
      "      108    0.001    0.000    0.002    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/common.py:348(not_supported_reasons)\n",
      "       19    0.000    0.000    0.002    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:248(forward)\n",
      "     1019    0.002    0.000    0.002    0.000 {method 'reshape' of 'torch._C.TensorBase' objects}\n",
      "       15    0.001    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/model/diffusion/transformer_for_action_diffusion.py:31(timestep_embedding)\n",
      "       36    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/cutlass.py:202(apply)\n",
      "      396    0.001    0.000    0.001    0.000 {method 'unbind' of 'torch._C.TensorBase' objects}\n",
      "      721    0.001    0.000    0.001    0.000 {method 'permute' of 'torch._C.TensorBase' objects}\n",
      "       36    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/cutlass.py:266(apply_bmhk)\n",
      "       81    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:216(forward)\n",
      "     5037    0.001    0.000    0.001    0.000 {built-in method torch._C._get_tracing_state}\n",
      "      144    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:496(get_device_capability)\n",
      "       36    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/flash3.py:283(not_supported_reasons)\n",
      "       36    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/cutlass.py:317(not_supported_reasons)\n",
      "       81    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/functional.py:2879(layer_norm)\n",
      "      144    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:513(get_device_properties)\n",
      "        1    0.000    0.000    0.001    0.001 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:384(forward)\n",
      "       36    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/_ops.py:1105(__call__)\n",
      "       36    0.001    0.000    0.001    0.000 {built-in method torch._ops.aten._efficient_attention_forward}\n",
      "        4    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:750(forward)\n",
      "       81    0.001    0.000    0.001    0.000 {built-in method torch.layer_norm}\n",
      "       36    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/unbind.py:115(unbind)\n",
      "       36    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:611(not_supported_reasons)\n",
      "       36    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/autograd/function.py:559(apply)\n",
      "      831    0.001    0.000    0.001    0.000 {built-in method torch.dropout}\n",
      "       72    0.001    0.000    0.001    0.000 /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/layer_scale.py:26(forward)\n",
      "      144    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/cuda/_utils.py:9(_get_device_index)\n",
      "       15    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/activation.py:431(forward)\n",
      "       36    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/common.py:120(validate_inputs)\n",
      "       15    0.000    0.000    0.001    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/functional.py:2358(silu)\n",
      "       36    0.000    0.000    0.000    0.000 {built-in method apply}\n",
      "       15    0.000    0.000    0.000    0.000 {built-in method torch._C._nn.silu}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method torch._transformer_encoder_layer_fwd}\n",
      "       34    0.000    0.000    0.000    0.000 {built-in method torch.cat}\n",
      "        2    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:269(forward)\n",
      "        2    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:327(normalize)\n",
      "       36    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/cutlass.py:49(_minimum_gemm_alignment)\n",
      "      831    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/_VF.py:27(__getattr__)\n",
      "        2    0.000    0.000    0.000    0.000 /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:213(prepare_tokens_with_masks)\n",
      "        2    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:905(normalize)\n",
      "       76    0.000    0.000    0.000    0.000 {built-in method builtins.any}\n",
      "      182    0.000    0.000    0.000    0.000 {method 'transpose' of 'torch._C.TensorBase' objects}\n",
      "        3    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:553(forward)\n",
      "        3    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:536(_conv_forward)\n",
      "        2    0.000    0.000    0.000    0.000 /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/patch_embed.py:68(forward)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method torch.conv2d}\n",
      "      144    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/_utils.py:773(_get_device_index)\n",
      "     2720    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/_jit_internal.py:103(is_scripting)\n",
      "       36    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/unbind.py:86(forward)\n",
      "      144    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/common.py:482(check_lastdim_alignment_stride1)\n",
      "1778/1777    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "       15    0.000    0.000    0.000    0.000 {built-in method torch.arange}\n",
      "       48    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:822(<genexpr>)\n",
      "       76    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
      "     1030    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "1409/1405    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "       36    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/_functorch/utils.py:32(unwrap_dead_wrappers)\n",
      "      846    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_unary}\n",
      "      108    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/common.py:331(shape_not_supported_reasons)\n",
      "       36    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py:79(_dispatch_fw_priority_list)\n",
      "      144    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:289(_lazy_init)\n",
      "      144    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:896(device_count)\n",
      "       48    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2751(modules)\n",
      "       15    0.000    0.000    0.000    0.000 {built-in method torch.cos}\n",
      "       15    0.000    0.000    0.000    0.000 {built-in method torch.sin}\n",
      "       15    0.000    0.000    0.000    0.000 {built-in method torch.exp}\n",
      "      144    0.000    0.000    0.000    0.000 {built-in method torch.cuda._get_device_properties}\n",
      "       32    0.000    0.000    0.000    0.000 {method 'expand' of 'torch._C.TensorBase' objects}\n",
      "   138/54    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2778(named_modules)\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/model/vision/obs_encoder.py:201(forward_state)\n",
      "      108    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/_functorch/utils.py:34(<genexpr>)\n",
      "      324    0.000    0.000    0.000    0.000 {method 'stride' of 'torch._C.TensorBase' objects}\n",
      "      108    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/common.py:32(is_available)\n",
      "       17    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/autograd/grad_mode.py:84(__exit__)\n",
      "      144    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:242(is_initialized)\n",
      "       34    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/autograd/grad_mode.py:184(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method torch.as_tensor}\n",
      "       36    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/common.py:96(normalize_bmhk)\n",
      "       36    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/cutlass.py:136(_custom_mask_type)\n",
      "      144    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:107(_is_compiled)\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/einops/layers/torch.py:13(forward)\n",
      "       72    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:493(_check_needs_no_topleft)\n",
      "      144    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/common.py:131(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/einops/_torch_specific.py:74(apply_for_scriptable_torch)\n",
      "       17    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/autograd/grad_mode.py:80(__enter__)\n",
      "      144    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/common.py:122(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/model/common/normalizer.py:55(__getitem__)\n",
      "       72    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/cutlass.py:87(_get_tensor_bias)\n",
      "      180    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/common.py:71(device)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'clone' of 'torch._C.TensorBase' objects}\n",
      "       81    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/backends/__init__.py:38(__get__)\n",
      "       56    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:854(<genexpr>)\n",
      "       36    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/cutlass.py:98(_check_bias_alignment)\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/model/common/normalizer.py:183(unnormalize)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'any' of 'torch._C.TensorBase' objects}\n",
      "        2    0.000    0.000    0.000    0.000 /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:311(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/model/common/normalizer.py:284(_normalize)\n",
      "      144    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py:466(<genexpr>)\n",
      "       17    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:154(__new__)\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/model/common/dict_of_tensor_mixin.py:5(__init__)\n",
      "      144    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/common.py:152(<genexpr>)\n",
      "       17    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/autograd/grad_mode.py:75(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'sub_' of 'torch._C.TensorBase' objects}\n",
      "      144    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/einops/einops.py:150(_reconstruct_from_shape_uncached)\n",
      "       15    0.000    0.000    0.000    0.000 {method 'unsqueeze' of 'torch._C.TensorBase' objects}\n",
      "       19    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:240(__iter__)\n",
      "       36    0.000    0.000    0.000    0.000 <string>:2(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:312(<listcomp>)\n",
      "      108    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:515(_check_strides_for_bmghk)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'div_' of 'torch._C.TensorBase' objects}\n",
      "        6    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2611(parameters)\n",
      "       36    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/cutlass.py:66(_get_seqlen_info)\n",
      "       16    0.000    0.000    0.000    0.000 {method 'to' of 'torch._C.TensorBase' objects}\n",
      "       36    0.000    0.000    0.000    0.000 {built-in method torch._C._functorch.unwrap_if_dead}\n",
      "      144    0.000    0.000    0.000    0.000 {built-in method torch._C._cuda_isInBadFork}\n",
      "       34    0.000    0.000    0.000    0.000 {built-in method torch._C._set_grad_enabled}\n",
      "       15    0.000    0.000    0.000    0.000 {method 'float' of 'torch._C.TensorBase' objects}\n",
      "       81    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function_variadic}\n",
      "       18    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:354(__iter__)\n",
      "      224    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "       30    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/__init__.py:1004(is_tensor)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method torch.randn}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "        2    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torchvision/utils.py:619(_log_api_usage_once)\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/model/common/module_attr_mixin.py:13(dtype)\n",
      "      182    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:46(forward)\n",
      "      108    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "        6    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2636(named_parameters)\n",
      "      145    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1935(__setattr__)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method torch.linspace}\n",
      "        6    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2591(_named_members)\n",
      "       81    0.000    0.000    0.000    0.000 {built-in method torch._C._get_cudnn_enabled}\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:473(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 {method 'flatten' of 'torch._C.TensorBase' objects}\n",
      "        2    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/src/dexgraspvla/controller/model/common/module_attr_mixin.py:9(device)\n",
      "       15    0.000    0.000    0.000    0.000 {built-in method math.log}\n",
      "       36    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/autograd/function.py:592(_is_setup_context_defined)\n",
      "       36    0.000    0.000    0.000    0.000 {built-in method torch._C._are_functorch_transforms_active}\n",
      "       55    0.000    0.000    0.000    0.000 {built-in method torch.is_grad_enabled}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'view' of 'torch._C.TensorBase' objects}\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:329(__getitem__)\n",
      "        3    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:789(__getitem__)\n",
      "        2    0.000    0.000    0.000    0.000 /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:313(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 /home/samsung/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/models/vision_transformer.py:179(interpolate_pos_encoding)\n",
      "       36    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py:27(_get_use_fa3)\n",
      "       36    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/xformers/ops/fmha/cutlass.py:41(_uses_tensorcores)\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method torch._C._log_api_usage_once}\n",
      "       41    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "       50    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/einops/_torch_specific.py:40(transpose)\n",
      "        2    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/jit/_trace.py:1323(is_tracing)\n",
      "        2    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:13(_assert_image_tensor)\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:312(_get_abs_string_index)\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/parameter.py:10(__instancecheck__)\n",
      "        2    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/einops/_torch_specific.py:68(reshape)\n",
      "       38    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/flatten.py:52(forward)\n",
      "       48    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        4    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:350(__len__)\n",
      "        5    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/backends/mha/__init__.py:9(get_fastpath_enabled)\n",
      "       17    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x561f0765c5e0}\n",
      "        6    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/_tensor.py:1121(__hash__)\n",
      "        5    0.000    0.000    0.000    0.000 {method 'size' of 'torch._C.TensorBase' objects}\n",
      "        2    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:9(_is_tensor_a_torch_image)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method torch.is_autocast_enabled}\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:47(_get_seq_len)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method torch._C._has_torch_function}\n",
      "        3    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/einops/einops.py:23(is_ellipsis_not_in_parenthesis)\n",
      "        2    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:500(__getitem__)\n",
      "       10    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/functional.py:5832(_canonical_mask)\n",
      "        3    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:778(_key_to_attr)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
      "        4    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/activation.py:1394(merge_masks)\n",
      "        3    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:2661(<lambda>)\n",
      "        3    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/einops/einops.py:29(_product)\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method torch._C._is_tracing}\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:825(__contains__)\n",
      "        5    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/functional.py:5860(_none_or_dtype)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'is_floating_point' of 'torch._C.TensorBase' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'dim' of 'torch._C.TensorBase' objects}\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/einops/_torch_specific.py:64(shape)\n",
      "        3    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/einops/einops.py:214(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:1158(_detect_is_causal_mask)\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1936(remove_from)\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 /data/shiqi/DexGraspVLA/.venv/lib/python3.10/site-packages/einops/einops.py:218(<dictcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {function _ParameterMeta.__instancecheck__ at 0x7f16e499b910}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _operator.index}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'values' of 'collections.OrderedDict' objects}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "action = vla_controller.predict_action(right_state[0:1,...],right_cam_img[0:1,...],rgbm_data[0:1,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7902ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "action.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
